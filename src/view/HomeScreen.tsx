import { GOOGLE_CLOUD_VISION_API_KEY, OPENAI_API_KEY } from '@env';
import { MaterialIcons } from '@expo/vector-icons';
import { Audio } from 'expo-av';
import { BlurView } from 'expo-blur';
import * as FileSystem from 'expo-file-system';
import * as ImageManipulator from 'expo-image-manipulator';
import type { ImagePickerAsset } from 'expo-image-picker';
import * as ImagePicker from 'expo-image-picker';
import { useCallback, useEffect, useRef, useState } from 'react';

import {
  ActionSheetIOS,
  Alert,
  Animated,
  Appearance,
  Image,
  Platform,
  ScrollView,
  Text,
  TouchableOpacity,
  View
} from 'react-native';
import Markdown from 'react-native-markdown-display';
import type { OcrResult } from '../api/googleVisionApi';
import { ocrWithGoogleVision } from '../api/googleVisionApi';
import { getInfoFromTextWithOpenAI, suggestTasksFromOcr, TaskSuggestion } from '../api/openaiApi';
import {
  answerQuestionFromSpeech
} from '../api/openaiApiForSTT';
import { speechToText, startRecording, stopRecording, textToSpeech } from '../api/speechApi';
import { extractTextFromVideo } from '../api/videoOcrApi';
import ImageTypeSelector from '../components/ImageTypeSelector';
import MediaPreviewModal from '../components/MediaPreviewModal';
import SummarizationSection from '../components/SummarizationSection';
import TaskSuggestionList from '../components/TaskSuggestionList';
import VideoPreview from '../components/VideoPreview';
import { ImageType } from '../constants/ImageTypes';
import { homeScreenStyles as styles } from '../styles/HomeScreen.styles';
import { detectImageType } from '../utils/imageTypeDetector';
import { translateToKorean } from '../utils/koreanTranslator';



interface SelectedImage {
  uri: string;
  width?: number;
  height?: number;
  assetId?: string;
  type?: 'image' | 'video'; 
}

interface OcrLoadingState {
  [uri: string]: boolean;
}

interface ImageTypeState {
  [uri: string]: ImageType;
}

interface AnalysisResult {
  text: string;
  objects: Array<{
    name: string;
    confidence: number;
    boundingBox: Array<{ x: number; y: number }>;
  }>;
  labels: Array<{
    description: string;
    confidence: number;
  }>;
  faces: Array<{
    joyLikelihood: string;
    sorrowLikelihood: string;
    angerLikelihood: string;
    surpriseLikelihood: string;
    underExposedLikelihood: string;
    blurredLikelihood: string;
    headwearLikelihood: string;
  }>;
  landmarks: Array<{
    description: string;
    score: number;
    locations: Array<{ x: number; y: number }>;
  }>;
  logos: Array<{
    description: string;
    score: number;
  }>;
  safeSearch: {
    adult: string;
    spoof: string;
    medical: string;
    violence: string;
    racy: string;
  };
  colors: Array<{
    color: { red: number; green: number; blue: number };
    score: number;
    pixelFraction: number;
  }>;
  webEntities: Array<{
    description: string;
    score: number;
  }>;
  similarImages: Array<{
    url: string;
    score: number;
  }>;
}

// ì´ë¯¸ì§€ë³„ task ì œì•ˆì„ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤
interface ImageTaskSuggestions {
  [imageUri: string]: {
    suggestions: TaskSuggestion[];
    imageType: ImageType;
  }
}

// ë¡œê¹… í—¬í¼ í•¨ìˆ˜ë“¤ì„ HomeScreen ì»´í¬ë„ŒíŠ¸ ì™¸ë¶€ë¡œ ì´ë™
const logSection = (title: string) => {
  console.log('\n' + '='.repeat(50));
  console.log(`ğŸ“Œ ${title}`);
  console.log('='.repeat(50));
};

const logInfo = (message: string, data?: any) => {
  if (data) {
    console.log(`â„¹ï¸ ${message}:`, data);
  } else {
    console.log(`â„¹ï¸ ${message}`);
  }
};

const logError = (message: string, error?: any) => {
  if (error) {
    console.error(`âŒ ${message}:`, error);
  } else {
    console.error(`âŒ ${message}`);
  }
};

const LoadingWave = () => {
  const [animations] = useState([
    new Animated.Value(0),
    new Animated.Value(0),
    new Animated.Value(0),
    new Animated.Value(0),
  ]);

  useEffect(() => {
    const animate = () => {
      const sequences = animations.map((anim, index) => {
        return Animated.sequence([
          Animated.timing(anim, {
            toValue: 1,
            duration: 400,
            delay: index * 100,
            useNativeDriver: false,
          }),
          Animated.timing(anim, {
            toValue: 0,
            duration: 400,
            useNativeDriver: false,
          }),
        ]);
      });

      Animated.stagger(100, sequences).start(() => animate());
    };

    animate();
  }, []);

  return (
    <View style={styles.loadingWaveContainer}>
      {animations.map((anim, index) => (
        <Animated.View
          key={index}
          style={[
            styles.loadingBar,
            {
              opacity: anim,
              transform: [
                {
                  scaleY: anim.interpolate({
                    inputRange: [0, 1],
                    outputRange: [0.5, 1],
                  }),
                },
              ],
            },
          ]}
        />
      ))}
    </View>
  );
};

const AnswerLoadingSkeleton = () => {
  const [animations] = useState([
    new Animated.Value(0),
    new Animated.Value(0),
    new Animated.Value(0),
    new Animated.Value(0),
  ]);

  useEffect(() => {
    const animate = () => {
      const sequences = animations.map((anim, index) => {
        return Animated.sequence([
          Animated.timing(anim, {
            toValue: 1,
            duration: 1000,
            delay: index * 200,
            useNativeDriver: false,
          }),
          Animated.timing(anim, {
            toValue: 0,
            duration: 1000,
            useNativeDriver: false,
          }),
        ]);
      });

      Animated.stagger(200, sequences).start(() => animate());
    };

    animate();
  }, []);

  return (
    <View style={styles.answerLoadingContainer}>
      {animations.map((anim, index) => (
        <Animated.View
          key={index}
          style={[
            styles.answerLoadingBar,
            index === 0 && styles.answerLoadingBarLong,
            index === 1 && styles.answerLoadingBarMedium,
            index === 2 && styles.answerLoadingBarShort,
            index === 3 && styles.answerLoadingBarMedium,
            {
              opacity: anim.interpolate({
                inputRange: [0, 1],
                outputRange: [0.3, 0.7],
              }),
            },
          ]}
        />
      ))}
    </View>
  );
};

const OcrLoadingAnimation = () => {
  const [animations] = useState([
    new Animated.Value(0),
    new Animated.Value(0),
    new Animated.Value(0),
    new Animated.Value(0),
  ]);

  useEffect(() => {
    const animate = () => {
      const sequences = animations.map((anim, index) => {
        return Animated.sequence([
          Animated.timing(anim, {
            toValue: 1,
            duration: 400,
            delay: index * 100,
            useNativeDriver: false,
          }),
          Animated.timing(anim, {
            toValue: 0,
            duration: 400,
            useNativeDriver: false,
          }),
        ]);
      });

      Animated.stagger(100, sequences).start(() => animate());
    };

    animate();
  }, []);

  return (
    <View style={styles.loadingOverlayThumb}>
      {animations.map((anim, index) => (
        <Animated.View
          key={index}
          style={[
            styles.loadingBar,
            {
              opacity: anim,
              transform: [
                {
                  scaleY: anim.interpolate({
                    inputRange: [0, 1],
                    outputRange: [0.5, 1],
                  }),
                },
              ],
            },
          ]}
        />
      ))}
    </View>
  );
};

// ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼ ì •ì˜
const markdownStyles = {
  body: {
    color: '#000',
    fontSize: 16,
    lineHeight: 24,
  },
  heading1: {
    fontSize: 24,
    fontWeight: 'bold',
    marginBottom: 16,
    color: '#000',
  },
  heading2: {
    fontSize: 20,
    fontWeight: 'bold',
    marginBottom: 14,
    color: '#000',
  },
  heading3: {
    fontSize: 18,
    fontWeight: 'bold',
    marginBottom: 12,
    color: '#000',
  },
  paragraph: {
    marginBottom: 12,
  },
  list_item: {
    marginBottom: 8,
  },
  bullet_list: {
    marginBottom: 12,
  },
  ordered_list: {
    marginBottom: 12,
  },
  code_inline: {
    backgroundColor: '#f0f0f0',
    padding: 4,
    borderRadius: 4,
    fontFamily: Platform.OS === 'ios' ? 'Menlo' : 'monospace',
  },
  code_block: {
    backgroundColor: '#f0f0f0',
    padding: 12,
    borderRadius: 4,
    marginBottom: 12,
    fontFamily: Platform.OS === 'ios' ? 'Menlo' : 'monospace',
  },
  blockquote: {
    borderLeftWidth: 4,
    borderLeftColor: '#ddd',
    paddingLeft: 12,
    marginBottom: 12,
    fontStyle: 'italic',
  },
  link: {
    color: '#4299E2',
    textDecorationLine: 'underline' as const,
  },
  strong: {
    fontWeight: 'bold',
  },
  em: {
    fontStyle: 'italic',
  },
} as const;

const HomeScreen = () => {
  const [colorScheme, setColorScheme] = useState(Appearance.getColorScheme());
  
  useEffect(() => {
    const subscription = Appearance.addChangeListener(({ colorScheme }) => {
      setColorScheme(colorScheme);
    });

    return () => subscription.remove();
  }, []);

  const [selectedImages, setSelectedImages] = useState<ImagePickerAsset[]>([]);
  const [infoResult, setInfoResult] = useState<string | null>(null);
  const [ocrResults, setOcrResults] = useState<{[uri: string]: OcrResult | null}>({});
  const [isLoadingOcr, setIsLoadingOcr] = useState<OcrLoadingState>({});
  const [questionText, setQuestionText] = useState<string>('');
  const [previewMediaAsset, setPreviewMediaAsset] = useState<ImagePickerAsset | null>(null);
  const [isFetchingInfo, setIsFetchingInfo] = useState<boolean>(false);
  const [cameraPermissionStatus, setCameraPermissionStatus] = useState<ImagePicker.PermissionStatus | null>(null);
  const [assetUriMap, setAssetUriMap] = useState<{ [internalUri: string]: string | undefined }>({});
  const [imageTypes, setImageTypes] = useState<ImageTypeState>({});
  const [fadeAnim] = useState(new Animated.Value(0));
  const [analysisResult, setAnalysisResult] = useState<AnalysisResult | null>(null);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [selectedObject, setSelectedObject] = useState<number | null>(null);
  const [imageDimensions, setImageDimensions] = useState({ width: 0, height: 0 });
  const [error, setError] = useState<string | null>(null);
  const [searchTerm, setSearchTerm] = useState('');
  const [taskSuggestions, setTaskSuggestions] = useState<TaskSuggestion[]>([]);
  const [imageTaskSuggestions, setImageTaskSuggestions] = useState<ImageTaskSuggestions>({});
  const [selectedImageUri, setSelectedImageUri] = useState<string | null>(null);

  const [recording, setRecording] = useState<Audio.Recording | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessingSpeech, setIsProcessingSpeech] = useState(false);
  const [ttsAudioUri, setTtsAudioUri] = useState<string | null>(null);
  const [isPlayingAudio, setIsPlayingAudio] = useState(false);
  const [isProcessingAI, setIsProcessingAI] = useState(false);

  // ìŒì„± ëª…ë ¹ ì²˜ë¦¬ ê´€ë ¨ ìƒíƒœ ì¶”ê°€
  const recordingDuration = useRef<number>(0);
  const recordingStartTime = useRef<number>(0);

  // ë…¹ìŒ ê´€ë ¨ ì°¸ì¡° ì¶”ê°€
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const recordingStartTimeRef = useRef<number>(0);
  const isCleaningUpRef = useRef<boolean>(false);

  // ì•ˆì „í•œ ë…¹ìŒ ê°ì²´ ì •ë¦¬ í•¨ìˆ˜
  const cleanupRecording = useCallback(async () => {
    if (isCleaningUpRef.current) return;
    isCleaningUpRef.current = true;
    
    try {
      if (recordingRef.current) {
        console.log('ë…¹ìŒ ê°ì²´ ì •ë¦¬ ì‹œì‘');
        const recording = recordingRef.current;
        recordingRef.current = null;
        
        try {
          const status = await recording.getStatusAsync();
          if (status.canRecord || status.isRecording) {
            await recording.stopAndUnloadAsync();
          } else {
            await recording.stopAndUnloadAsync();
          }
        } catch (error) {
          console.log('ë…¹ìŒ ê°ì²´ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨):', error);
        }
        
        console.log('ë…¹ìŒ ê°ì²´ ì •ë¦¬ ì™„ë£Œ');
      }
    } catch (error) {
      console.error('ë…¹ìŒ ì •ë¦¬ ì¤‘ ì˜ˆì™¸:', error);
    } finally {
      isCleaningUpRef.current = false;
    }
  }, []);

  // ì˜¤ë””ì˜¤ ì¤‘ì§€ í•¨ìˆ˜
  const stopCurrentAudio = useCallback(async () => {
    try {
      if (soundRef.current) {
        await soundRef.current.stopAsync();
        await soundRef.current.unloadAsync();
        soundRef.current = null;
      }
      setIsPlayingAudio(false);
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜:', error);
      setIsPlayingAudio(false);
    }
  }, []);

  // ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜
  const resetStates = useCallback(async () => {
    setIsRecording(false);
    setIsProcessingSpeech(false);
    setIsPlayingAudio(false);
    setIsProcessingAI(false);
    await cleanupRecording();
    await stopCurrentAudio();
  }, [cleanupRecording, stopCurrentAudio]);

  // ìŒì„± ë…¹ìŒ ì‹œì‘
  const handleStartRecording = useCallback(async () => {
    try {
      if (isRecording || isProcessingSpeech || isProcessingAI || isCleaningUpRef.current) {
        console.log('ë…¹ìŒ ì‹œì‘ ë¬´ì‹œ - ì´ë¯¸ ì§„í–‰ ì¤‘');
        return;
      }
      
      console.log('ë…¹ìŒ ì‹œì‘ ì‹œë„');
      
      await cleanupRecording();
      await stopCurrentAudio();
      
      recordingStartTimeRef.current = Date.now();
      
      setIsRecording(true);
      const newRecording = await startRecording();
      recordingRef.current = newRecording;
      
      console.log('ë…¹ìŒ ì‹œì‘ ì™„ë£Œ');
    } catch (error) {
      console.error('ë…¹ìŒ ì‹œì‘ ì˜¤ë¥˜:', error);
      setIsRecording(false);
      await cleanupRecording();
    }
  }, [isRecording, isProcessingSpeech, isProcessingAI, cleanupRecording, stopCurrentAudio]);

  // ìŒì„± ë…¹ìŒ ì¤‘ì§€ ë° ì²˜ë¦¬
  const handleStopRecording = useCallback(async () => {
    try {
      if (!isRecording || !recordingRef.current || isCleaningUpRef.current) {
        console.log('ë…¹ìŒ ì¤‘ì§€ ë¬´ì‹œ - ë…¹ìŒ ì¤‘ì´ ì•„ë‹˜');
        setIsRecording(false);
        return;
      }
      
      const recordingDuration = Date.now() - recordingStartTimeRef.current;
      console.log(`ë…¹ìŒ ì‹œê°„: ${recordingDuration}ms`);
      
      if (recordingDuration < 500) {
        console.log('ë…¹ìŒì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ë‹¨');
        setIsRecording(false);
        await cleanupRecording();
        return;
      }
      
      setIsRecording(false);
      setIsProcessingSpeech(true);
      
      const recording = recordingRef.current;
      recordingRef.current = null;
      
      const audioUri = await stopRecording(recording);
      
      console.log('ë…¹ìŒ ì¤‘ì§€ ì™„ë£Œ, STT ì²˜ë¦¬ ì‹œì‘');
      
      const transcribedText = await speechToText(audioUri);
      
      if (!transcribedText || transcribedText.trim() === '' || transcribedText === 'ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.') {
        console.log('ìŒì„± ì¸ì‹ ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ì—†ìŒ');
        setIsProcessingSpeech(false);
        return;
      }
      
      console.log('ë³€í™˜ëœ í…ìŠ¤íŠ¸:', transcribedText);
      
      // í˜„ì¬ ì„ íƒëœ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
      if (selectedImageUri) {
        const selectedMedia = selectedImages.find(img => img.uri === selectedImageUri);
        if (selectedMedia) {
          const currentOcrResult = ocrResults[selectedImageUri];
          if (currentOcrResult) {
            setIsProcessingAI(true);
            
            // ì„ íƒëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            const currentAnalysisResult = await analyzeImage(selectedImageUri);
            
            const aiResponse = await answerQuestionFromSpeech(
              transcribedText.trim(),
              currentOcrResult.fullText,
              currentAnalysisResult
            );
            
            const ttsUri = await textToSpeech(aiResponse);
            if (ttsUri) {
              setIsPlayingAudio(true);
              const { sound } = await Audio.Sound.createAsync(
                { uri: ttsUri },
                { shouldPlay: true }
              );
              
              soundRef.current = sound;
              sound.setOnPlaybackStatusUpdate((status) => {
                if (status.isLoaded && status.didJustFinish) {
                  sound.unloadAsync();
                  soundRef.current = null;
                  setIsPlayingAudio(false);
                }
              });
            }
          } else {
            console.log('ì„ íƒëœ ì´ë¯¸ì§€ì— ëŒ€í•œ OCR ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.');
            Alert.alert('ì•Œë¦¼', 'ì„ íƒëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.');
          }
        }
      } else {
        console.log('ì„ íƒëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.');
        Alert.alert('ì•Œë¦¼', 'ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.');
      }
      
    } catch (error) {
      console.error('ë…¹ìŒ ì¤‘ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
      await resetStates();
    } finally {
      setIsProcessingSpeech(false);
      setIsProcessingAI(false);
    }
  }, [isRecording, cleanupRecording, resetStates, selectedImageUri, selectedImages, ocrResults]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      console.log('ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬');
      resetStates();
    };
  }, [resetStates]);

  // ë””ë²„ê¹…ì„ ìœ„í•œ useEffect ì¶”ê°€
  useEffect(() => {
    console.log('Task suggestions updated:', taskSuggestions);
  }, [taskSuggestions]);

  const handleTypeChange = (uri: string, newType: ImageType) => {
    setImageTypes(prev => ({ ...prev, [uri]: newType }));
  };

  const processImageWithOCR = async (imageUri: string) => {
    setIsLoadingOcr(prev => ({ ...prev, [imageUri]: true }));
    try {
      logSection('ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ë¶„ì„');
      logInfo('ì´ë¯¸ì§€ URI', imageUri);

      const ocrResult = await ocrWithGoogleVision(imageUri);
      
      if (ocrResult && ocrResult.textBoxes.length > 0) {
        logSection('í…ìŠ¤íŠ¸ ì¸ì‹ ê²°ê³¼');
        logInfo('ê°ì§€ëœ í…ìŠ¤íŠ¸', ocrResult.fullText);
        logInfo('í…ìŠ¤íŠ¸ ë°•ìŠ¤ ìˆ˜', ocrResult.textBoxes.length);

        setOcrResults(prevResults => ({ ...prevResults, [imageUri]: ocrResult }));
        const detectedType = detectImageType(ocrResult.fullText);
        setImageTypes(prev => ({ ...prev, [imageUri]: detectedType }));

        // ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ë¡œê¹…
        const analysisResult = await analyzeImage(imageUri);
        if (analysisResult) {
          logSection('ë¬¼ì²´ ì¸ì‹ ê²°ê³¼');
          if (analysisResult.objects.length > 0) {
            const objectsWithKorean = await Promise.all(
              analysisResult.objects.map(async obj => {
                const koreanTranslations = await translateToKorean(obj.name);
                if (koreanTranslations.length > 0) {
                  return {
                    name: obj.name,
                    korean: koreanTranslations.join(', ')
                  };
                }
                return null;
              })
            );
            const filteredObjects = objectsWithKorean.filter(obj => obj !== null);
            
            // ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ Map ì‚¬ìš©
            const uniqueObjects = new Map();
            filteredObjects.forEach(obj => {
              if (obj) {
                uniqueObjects.set(obj.name, obj);
              }
            });
            
            if (uniqueObjects.size > 0) {
              logInfo('ê°ì§€ëœ ë¬¼ì²´', Array.from(uniqueObjects.values()));
            }
          }
          if (analysisResult.labels.length > 0) {
            logInfo('ê°ì§€ëœ ë¼ë²¨', analysisResult.labels.map(label => ({
              description: label.description,
              confidence: `${(label.confidence * 100).toFixed(1)}%`
            })));
          }
          logInfo('ì¸ì‹ ì™„ë£Œ');
        }

        const suggestions = await suggestTasksFromOcr(ocrResult.fullText);
        if (suggestions && suggestions.length > 0) {
          setImageTaskSuggestions(prev => ({
            ...prev,
            [imageUri]: {
              suggestions,
              imageType: detectedType
            }
          }));
        }
      } else {
        logInfo('í…ìŠ¤íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ');
        setOcrResults(prevResults => ({ ...prevResults, [imageUri]: null }));
        setImageTypes(prev => ({ ...prev, [imageUri]: 'OTHER' }));
      }
    } catch (error) {
      logError(`ì´ë¯¸ì§€ OCR ì˜¤ë¥˜ (${imageUri})`, error);
      setOcrResults(prevResults => ({ ...prevResults, [imageUri]: null }));
      setImageTypes(prev => ({ ...prev, [imageUri]: 'OTHER' }));
    } finally {
      setIsLoadingOcr(prev => ({ ...prev, [imageUri]: false }));
    }
  };

  const processVideoWithOCR = async (videoUri: string) => {
    setIsLoadingOcr(prev => ({ ...prev, [videoUri]: true }));
    try {
      const results = await extractTextFromVideo(videoUri, 1); // 1ì´ˆë‹¹ 1í”„ë ˆì„
      console.log('Video OCR Results:', results);

      if (results.length > 0) {
        // ëª¨ë“  í”„ë ˆì„ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨
        const combinedText = results
          .map(result => `[${result.time/1000}ì´ˆ] ${result.text}`)
          .join('\n\n');
        
        // OcrResult í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        const ocrResult: OcrResult = {
          fullText: combinedText,
          textBoxes: results.map(result => ({
            description: result.text,
            boundingPoly: {
              vertices: [
                { x: 0, y: 0 },
                { x: 0, y: 0 },
                { x: 0, y: 0 },
                { x: 0, y: 0 }
              ]
            }
          }))
        };

        setOcrResults(prevResults => ({ ...prevResults, [videoUri]: ocrResult }));
        const detectedType = detectImageType(combinedText);
        setImageTypes(prev => ({ ...prev, [videoUri]: detectedType }));

        // OCR ê²°ê³¼ë¡œ task ì œì•ˆ ìƒì„±
        const suggestions = await suggestTasksFromOcr(combinedText);
        if (suggestions && suggestions.length > 0) {
          setImageTaskSuggestions(prev => ({
            ...prev,
            [videoUri]: {
              suggestions,
              imageType: detectedType
            }
          }));
        }
      } else {
        setOcrResults(prevResults => ({ ...prevResults, [videoUri]: null }));
        setImageTypes(prev => ({ ...prev, [videoUri]: 'OTHER' }));
      }
    } catch (error) {
      console.error(`ë¹„ë””ì˜¤ OCR ì˜¤ë¥˜ (${videoUri}):`, error);
      setOcrResults(prevResults => ({ ...prevResults, [videoUri]: null }));
      setImageTypes(prev => ({ ...prev, [videoUri]: 'OTHER' }));
    } finally {
      // ì‘ì—… ì œì•ˆì´ ì™„ë£Œëœ í›„ì— ë¡œë”© ìƒíƒœ í•´ì œ
      setIsLoadingOcr(prev => ({ ...prev, [videoUri]: false }));
    }
  };

  useEffect(() => {
    if (!OPENAI_API_KEY || !GOOGLE_CLOUD_VISION_API_KEY) {
      Alert.alert(
        'API í‚¤ ì˜¤ë¥˜',
        'OpenAI ë˜ëŠ” Google Cloud Vision API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.',
      );
    }
    (async () => {
      const permission = await ImagePicker.requestCameraPermissionsAsync();
      setCameraPermissionStatus(permission.status);
    })();
  }, []);

  const verifyCameraPermissions = async () => {
    if (cameraPermissionStatus === ImagePicker.PermissionStatus.UNDETERMINED) { 
      const permissionResponse = await ImagePicker.requestCameraPermissionsAsync();
      setCameraPermissionStatus(permissionResponse.status);
      return permissionResponse.granted;
    }
    if (cameraPermissionStatus === ImagePicker.PermissionStatus.DENIED) { 
      Alert.alert('ê¶Œí•œ í•„ìš”', 'ì¹´ë©”ë¼ ì‚¬ìš©ì„ ìœ„í•´ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
      return false;
    }
    return true; 
  };

  const showMediaOptions = () => {
    if (Platform.OS === 'ios') {
      ActionSheetIOS.showActionSheetWithOptions(
        {
          options: ['ì·¨ì†Œ', 'ì‚¬ì§„ ë³´ê´€í•¨', 'ì‚¬ì§„ ì°ê¸°', 'íŒŒì¼ ì„ íƒ'],
          cancelButtonIndex: 0,
        },
        (buttonIndex) => {
          if (buttonIndex === 1) {
            handleChooseMedia();
          } else if (buttonIndex === 2) {
            handleTakePhoto();
          } else if (buttonIndex === 3) {
            handleChooseDocument();
          }
        }
      );
    } else {
      // Androidìš© Alert ë‹¤ì´ì–¼ë¡œê·¸
      Alert.alert(
        'ë¯¸ë””ì–´ ì„ íƒ',
        'ì›í•˜ëŠ” ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”',
        [
          { text: 'ì·¨ì†Œ', style: 'cancel' },
          { text: 'ì‚¬ì§„ ë³´ê´€í•¨', onPress: () => handleChooseMedia() },
          { text: 'ì‚¬ì§„ ì°ê¸°', onPress: () => handleTakePhoto() },
          { text: 'íŒŒì¼ ì„ íƒ', onPress: () => handleChooseDocument() },
        ],
        { cancelable: true }
      );
    }
  };

  const handleChooseMedia = async () => {
    try {
      const result = await ImagePicker.launchImageLibraryAsync({
        mediaTypes: ImagePicker.MediaTypeOptions.All,
        allowsMultipleSelection: true,
        quality: 1,
      });

      if (!result.canceled && result.assets) {
        // ì •ë°©í–¥ ë³€í™˜ ì ìš© (ì´ë¯¸ì§€ì—ë§Œ)
        const manipulatedAssets = await Promise.all(result.assets.map(async asset => {
          if (asset.type === 'image' && asset.uri) {
            const manipulated = await ImageManipulator.manipulateAsync(
              asset.uri,
              [], // no-op, just strip EXIF
              { compress: 1, format: ImageManipulator.SaveFormat.JPEG }
            );
            return { ...asset, uri: manipulated.uri };
          }
          return asset;
        }));

        // ì„ íƒëœ ì´ë¯¸ì§€ë“¤ì„ ìƒíƒœì— ì¶”ê°€
        setSelectedImages(prevImages => [...prevImages, ...manipulatedAssets]);
        const newAssetUriMap = { ...assetUriMap };
        manipulatedAssets.forEach(asset => {
          newAssetUriMap[asset.uri] = asset.assetId ?? undefined;
        });
        setAssetUriMap(newAssetUriMap);

        // ê° ë¯¸ë””ì–´ì— ëŒ€í•´ OCR ì²˜ë¦¬ ë° task ì œì•ˆ ìƒì„±
        for (const asset of manipulatedAssets) {
          if (asset.uri) {
            if (asset.type === 'video') {
              await processVideoWithOCR(asset.uri);
            } else {
              await processImageWithOCR(asset.uri);
            }
          }
        }
      }
    } catch (error) {
      console.error('ë¯¸ë””ì–´ ì„ íƒ ì˜¤ë¥˜:', error);
      Alert.alert('ì˜¤ë¥˜', 'ë¯¸ë””ì–´ë¥¼ ì„ íƒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  };

  const handleChooseDocument = async () => {
    try {
      // ë¬¸ì„œ ì„ íƒ ê¸°ëŠ¥ì€ Expoì˜ DocumentPickerë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ,
      // í˜„ì¬ í”„ë¡œì íŠ¸ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì•„ ì•Œë¦¼ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
      Alert.alert(
        'ì•Œë¦¼',
        'ë¬¸ì„œ ì„ íƒ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ expo-document-picker íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.',
        [{ text: 'í™•ì¸', onPress: () => console.log('ë¬¸ì„œ ì„ íƒ ê¸°ëŠ¥ í•„ìš”') }]
      );
      
      // ì‹¤ì œ êµ¬í˜„ì€ ì•„ë˜ì™€ ê°™ì´ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
      // import * as DocumentPicker from 'expo-document-picker';
      // const result = await DocumentPicker.getDocumentAsync({ type: '*/*' });
      // if (result.type === 'success') {
      //   // ì„ íƒëœ ë¬¸ì„œ ì²˜ë¦¬
      // }
    } catch (error) {
      console.error('ë¬¸ì„œ ì„ íƒ ì˜¤ë¥˜:', error);
      Alert.alert('ì˜¤ë¥˜', 'ë¬¸ì„œë¥¼ ì„ íƒí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  };

  const handleTakePhoto = async () => {
    const hasPermission = await verifyCameraPermissions();
    if (!hasPermission) return;

    try {
      const result = await ImagePicker.launchCameraAsync({
        mediaTypes: ImagePicker.MediaTypeOptions.Images,
        allowsEditing: false,
        quality: 1,
      });

      if (!result.canceled && result.assets) {
        let newImage = result.assets[0];
        if (newImage.type === 'image' && newImage.uri) {
          const manipulated = await ImageManipulator.manipulateAsync(
            newImage.uri,
            [],
            { compress: 1, format: ImageManipulator.SaveFormat.JPEG }
          );
          newImage = { ...newImage, uri: manipulated.uri };
        }
        setSelectedImages(prevImages => [...prevImages, newImage]);
        const newAssetUriMap = { ...assetUriMap };
        newAssetUriMap[newImage.uri] = newImage.assetId ?? undefined;
        setAssetUriMap(newAssetUriMap);

        if (newImage.uri) {
          await processImageWithOCR(newImage.uri);
        }
      }
    } catch (error) {
      console.error('ì‚¬ì§„ ì´¬ì˜ ì˜¤ë¥˜:', error);
      Alert.alert('ì˜¤ë¥˜', 'ì‚¬ì§„ì„ ì´¬ì˜í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  };

  const handleGetInfo = async () => {
    if (selectedImages.length === 0) {
      Alert.alert('ì•Œë¦¼', 'ë¯¸ë””ì–´ë¥¼ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.');
      return;
    }

    setIsFetchingInfo(true);
    setInfoResult(null);

    try {
      const selectedMedia = selectedImages[0];
      if (!selectedMedia.uri) {
        throw new Error('ë¯¸ë””ì–´ URIê°€ ì—†ìŠµë‹ˆë‹¤.');
      }

      let analysisText = '';

      if (selectedMedia.type === 'video') {
        try {
          // ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
          const results = await extractTextFromVideo(selectedMedia.uri, 1);
          
          if (results.length > 0) {
            analysisText += '[ë¹„ë””ì˜¤ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼]\n';
            results.forEach(result => {
              analysisText += `[${result.time/1000}ì´ˆ] ${result.text}\n`;
            });
            analysisText += '\n';
          } else {
            analysisText += '[ë¹„ë””ì˜¤ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.]\n\n';
          }
        } catch (error) {
          console.error('ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜:', error);
          analysisText += '[ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.]\n\n';
        }
      } else {
        // ì´ë¯¸ì§€ ë¶„ì„ ë¡œì§
        const analysisResult = await analyzeImage(selectedMedia.uri);
        if (!analysisResult) {
          throw new Error('ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
        }

        console.log('=== Image Analysis Started ===');
        console.log('Analysis Result:', analysisResult);

        // ë¬¼ì²´ ê°ì§€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ìœ í˜• ì¶”ì •
        const detectedObjects = analysisResult.objects.map(obj => obj.name.toLowerCase());
        const detectedLabels = analysisResult.labels.map(label => label.description.toLowerCase());
        
        // ë¬¸ì„œ ìœ í˜• íŒë³„
        let documentType = '';
        if (detectedObjects.includes('receipt') || detectedLabels.includes('receipt')) {
          documentType = 'ì˜ìˆ˜ì¦';
        } else if (detectedObjects.includes('id card') || detectedLabels.includes('id card')) {
          documentType = 'ì‹ ë¶„ì¦';
        } else if (detectedObjects.includes('business card') || detectedLabels.includes('business card')) {
          documentType = 'ëª…í•¨';
        } else if (detectedObjects.includes('document') || detectedLabels.includes('document')) {
          documentType = 'ë¬¸ì„œ';
        }

        // ë¬¸ì„œ ìœ í˜•ì´ ê°ì§€ëœ ê²½ìš°
        if (documentType) {
          analysisText += `[ë¬¸ì„œ ìœ í˜•]\n${documentType}\n\n`;
          
          // ì˜ìˆ˜ì¦ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
          if (documentType === 'ì˜ìˆ˜ì¦') {
            const text = analysisResult.text;
            const totalAmountMatch = text.match(/ì´\s*[ê°€-í£]*\s*ê¸ˆì•¡\s*:?\s*(\d+[,\d]*ì›)/i) || 
                                   text.match(/í•©ê³„\s*:?\s*(\d+[,\d]*ì›)/i) ||
                                   text.match(/total\s*:?\s*(\d+[,\d]*ì›)/i);
            
            if (totalAmountMatch) {
              analysisText += `[ê²°ì œ ê¸ˆì•¡]\n${totalAmountMatch[1]}\n\n`;
            }
          }
        }

        // ê°ì§€ëœ ë¬¼ì²´ ì •ë³´
        if (analysisResult.objects.length > 0) {
          analysisText += '[ê°ì§€ëœ ë¬¼ì²´]\n';
          for (const obj of analysisResult.objects) {
            const vertices = obj.boundingBox;
            const minX = Math.min(...vertices.map(v => v.x));
            const minY = Math.min(...vertices.map(v => v.y));
            const maxX = Math.max(...vertices.map(v => v.x));
            const maxY = Math.max(...vertices.map(v => v.y));
            
            // ìœ„ì¹˜ ì •ë³´ë¥¼ ì´ë¯¸ì§€ í¬ê¸°ì— ëŒ€í•œ ìƒëŒ€ì  ë¹„ìœ¨ë¡œ í‘œì‹œ
            const position = {
              left: Math.round(minX * 100),
              top: Math.round(minY * 100),
              right: Math.round(maxX * 100),
              bottom: Math.round(maxY * 100)
            };
            
            // ì˜ì–´ ë‹¨ì–´ë¥¼ í•œê¸€ë¡œ ë²ˆì—­
            const koreanTranslations = await translateToKorean(obj.name);
            if (koreanTranslations.length > 0) {
              analysisText += `- ${obj.name} (${koreanTranslations.join(', ')})\n`;
              analysisText += `  ìœ„ì¹˜: ì™¼ìª½ ${position.left}%, ìœ„ ${position.top}%, ì˜¤ë¥¸ìª½ ${position.right}%, ì•„ë˜ ${position.bottom}%\n`;
            }
          }
          analysisText += '\n';
        }

        // í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        if (analysisResult.text) {
          analysisText += `[í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼]\n${analysisResult.text}\n\n`;
        }

        // ì´ë¯¸ì§€ ë¼ë²¨
        if (analysisResult.labels.length > 0) {
          analysisText += '[ì´ë¯¸ì§€ ë¼ë²¨]\n';
          analysisResult.labels.forEach(label => {
            analysisText += `- ${label.description} (ì‹ ë¢°ë„: ${Math.round(label.confidence * 100)}%)\n`;
          });
          analysisText += '\n';
        }

        // ì–¼êµ´ ê°ì§€ ê²°ê³¼
        if (analysisResult.faces.length > 0) {
          analysisText += '[ì–¼êµ´ ê°ì§€ ê²°ê³¼]\n';
          analysisResult.faces.forEach((face, index) => {
            analysisText += `ì–¼êµ´ ${index + 1}:\n`;
            if (face.joyLikelihood !== 'UNLIKELY') analysisText += `- ê¸°ì¨: ${face.joyLikelihood}\n`;
            if (face.sorrowLikelihood !== 'UNLIKELY') analysisText += `- ìŠ¬í””: ${face.sorrowLikelihood}\n`;
            if (face.angerLikelihood !== 'UNLIKELY') analysisText += `- ë¶„ë…¸: ${face.angerLikelihood}\n`;
            if (face.surpriseLikelihood !== 'UNLIKELY') analysisText += `- ë†€ëŒ: ${face.surpriseLikelihood}\n`;
            if (face.headwearLikelihood !== 'UNLIKELY') analysisText += `- ëª¨ì ì°©ìš©: ${face.headwearLikelihood}\n`;
          });
          analysisText += '\n';
        }

        // ëœë“œë§ˆí¬ ê°ì§€ ê²°ê³¼
        if (analysisResult.landmarks.length > 0) {
          analysisText += '[ê°ì§€ëœ ëœë“œë§ˆí¬]\n';
          analysisResult.landmarks.forEach(landmark => {
            analysisText += `- ${landmark.description} (ì‹ ë¢°ë„: ${Math.round(landmark.score * 100)}%)\n`;
          });
          analysisText += '\n';
        }

        // ë¡œê³  ê°ì§€ ê²°ê³¼
        if (analysisResult.logos.length > 0) {
          analysisText += '[ê°ì§€ëœ ë¡œê³ ]\n';
          analysisResult.logos.forEach(logo => {
            analysisText += `- ${logo.description} (ì‹ ë¢°ë„: ${Math.round(logo.score * 100)}%)\n`;
          });
          analysisText += '\n';
        }
      }

      if (questionText.trim()) {
        analysisText += `\nì§ˆë¬¸: ${questionText.trim()}`;
      }

      const information = await getInfoFromTextWithOpenAI(analysisText);
      setInfoResult(information);

      Animated.timing(fadeAnim, {
        toValue: 1,
        duration: 500,
        useNativeDriver: false,
      }).start();

    } catch (error) {
      console.error('Error processing media:', error);
      setInfoResult('ë¯¸ë””ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    } finally {
      setIsFetchingInfo(false);
    }
  };

  const analyzeImage = async (imageUri: string): Promise<AnalysisResult | null> => {
    try {
      // 1. ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (API í˜¸ì¶œ ì „ì— ë¯¸ë¦¬ ë¦¬ì‚¬ì´ì¦ˆ)
      const optimizedImage = await ImageManipulator.manipulateAsync(
        imageUri,
        [
          // ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¬ë©´ ë¦¬ì‚¬ì´ì¦ˆ (Google Vision API ê¶Œì¥: ìµœëŒ€ 20MB, ê¶Œì¥ 1024x1024)
          { resize: { width: Math.min(1024, 2048) } }
        ],
        { 
          compress: 0.8, // ì••ì¶•ë¥  ì¡°ì • (0.8 = 80% í’ˆì§ˆ)
          format: ImageManipulator.SaveFormat.JPEG 
        }
      );
  
      // 2. ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
      const base64Image = await FileSystem.readAsStringAsync(optimizedImage.uri, {
        encoding: FileSystem.EncodingType.Base64,
      });
  
      // 3. base64 í¬ê¸° ì²´í¬ (Google Vision API ì œí•œ: 20MB)
      const imageSizeInMB = (base64Image.length * 3) / 4 / (1024 * 1024); // base64 í¬ê¸° ê³„ì‚°
      console.log(`ì´ë¯¸ì§€ í¬ê¸°: ${imageSizeInMB.toFixed(2)}MB`);
      
      if (imageSizeInMB > 18) { // 18MBë¡œ ì—¬ìœ ë‘ê¸°
        console.warn('ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì¶”ê°€ ì••ì¶•ì„ ì§„í–‰í•©ë‹ˆë‹¤.');
        const furtherCompressed = await ImageManipulator.manipulateAsync(
          optimizedImage.uri,
          [{ resize: { width: 512 } }],
          { compress: 0.6, format: ImageManipulator.SaveFormat.JPEG }
        );
        
        const compressedBase64 = await FileSystem.readAsStringAsync(furtherCompressed.uri, {
          encoding: FileSystem.EncodingType.Base64,
        });
        
        return await callGoogleVisionAPI(compressedBase64);
      }
  
      return await callGoogleVisionAPI(base64Image);
  
    } catch (error) {
      console.log('ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
      
      // AbortError íŠ¹ë³„ ì²˜ë¦¬
      if (error instanceof Error && error.name === 'AbortError') {
        console.log('ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ë°œìƒ. ì¬ì‹œë„ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.');
        return null;
      }
      
      return null;
    }
  };
  
  // Google Vision API í˜¸ì¶œì„ ë³„ë„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
  const callGoogleVisionAPI = async (base64Image: string, retryCount: number = 0): Promise<AnalysisResult | null> => {
    const maxRetries = 2; // ìµœëŒ€ 2ë²ˆ ì¬ì‹œë„
    
    try {
      // Google Cloud Vision API ìš”ì²­ ë³¸ë¬¸ ì¤€ë¹„
      const requestBody = {
        requests: [
          {
            image: {
              content: base64Image,
            },
            features: [
              { type: 'TEXT_DETECTION' },
              { type: 'OBJECT_LOCALIZATION' },
              { type: 'FACE_DETECTION' },
              { type: 'LANDMARK_DETECTION' },
              { type: 'LOGO_DETECTION' },
              { type: 'SAFE_SEARCH_DETECTION' },
              { type: 'IMAGE_PROPERTIES' },
              { type: 'WEB_DETECTION' },
            ],
          },
        ],
      };
  
      // íƒ€ì„ì•„ì›ƒ ì„¤ì • (60ì´ˆë¡œ ì¦ê°€)
      const controller = new AbortController();
      const timeoutId = setTimeout(() => {
        console.log(`API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì‹œë„ íšŸìˆ˜: ${retryCount + 1})`);
        controller.abort();
      }, 60000); // 30ì´ˆ -> 60ì´ˆë¡œ ì¦ê°€
  
      // Google Cloud Vision API í˜¸ì¶œ
      const response = await fetch(
        `https://vision.googleapis.com/v1/images:annotate?key=${GOOGLE_CLOUD_VISION_API_KEY}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(requestBody),
          signal: controller.signal,
        }
      );
  
      clearTimeout(timeoutId);
  
      if (!response.ok) {
        console.log('Google Vision API ì‘ë‹µ ì˜¤ë¥˜:', response.status);
        
        // 429 (Rate Limit) ë˜ëŠ” 503 (Service Unavailable) ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
        if ((response.status === 429 || response.status === 503) && retryCount < maxRetries) {
          console.log(`API ì—ëŸ¬ ${response.status}. ${retryCount + 1}ì´ˆ í›„ ì¬ì‹œë„...`);
          await new Promise(resolve => setTimeout(resolve, (retryCount + 1) * 1000));
          return callGoogleVisionAPI(base64Image, retryCount + 1);
        }
        
        return null;
      }
  
      const data = await response.json();
      const result = data.responses[0];
  
      // ê²°ê³¼ ì²˜ë¦¬
      const analysisResult: AnalysisResult = {
        text: result.textAnnotations?.[0]?.description || '',
        objects: result.localizedObjectAnnotations?.map((obj: any) => ({
          name: obj.name,
          confidence: obj.score,
          boundingBox: obj.boundingPoly.normalizedVertices,
        })) || [],
        labels: result.labelAnnotations?.map((label: any) => ({
          description: label.description,
          confidence: label.score,
        })) || [],
        faces: result.faceAnnotations?.map((face: any) => ({
          joyLikelihood: face.joyLikelihood,
          sorrowLikelihood: face.sorrowLikelihood,
          angerLikelihood: face.angerLikelihood,
          surpriseLikelihood: face.surpriseLikelihood,
          boundingBox: face.boundingPoly.vertices,
        })) || [],
        landmarks: result.landmarkAnnotations?.map((landmark: any) => ({
          description: landmark.description,
          confidence: landmark.score,
          boundingBox: landmark.boundingPoly.vertices,
        })) || [],
        logos: result.logoAnnotations?.map((logo: any) => ({
          description: logo.description,
          confidence: logo.score,
          boundingBox: logo.boundingPoly.vertices,
        })) || [],
        safeSearch: result.safeSearchAnnotation || null,
        colors: result.imagePropertiesAnnotation?.dominantColors?.colors?.map((color: any) => ({
          color: color.color,
          score: color.score,
          pixelFraction: color.pixelFraction,
        })) || [],
        webEntities: result.webDetection?.webEntities?.map((entity: any) => ({
          description: entity.description,
          score: entity.score,
        })) || [],
        similarImages: result.webDetection?.visuallySimilarImages?.map((image: any) => ({
          url: image.url,
        })) || [],
      };
  
      return analysisResult;
  
    } catch (error) {
      console.log(`API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ (ì‹œë„ íšŸìˆ˜: ${retryCount + 1}):`, error);
      
      // AbortErrorì¸ ê²½ìš° ì¬ì‹œë„
      if (error instanceof Error && error.name === 'AbortError' && retryCount < maxRetries) {
        console.log(`íƒ€ì„ì•„ì›ƒ ë°œìƒ. ${retryCount + 1}ì´ˆ í›„ ì¬ì‹œë„...`);
        await new Promise(resolve => setTimeout(resolve, (retryCount + 1) * 1000));
        return callGoogleVisionAPI(base64Image, retryCount + 1);
      }
      
      return null;
    }
  };

  const handleMediaPreview = async (media: ImagePickerAsset) => {
    setPreviewMediaAsset(media);
    if (media.type === 'image') {
      try {
        const result = await analyzeImage(media.uri);
        setAnalysisResult(result);
      } catch (error) {
        console.error('Error analyzing image:', error);
      }
    }
  };

  // ì´ë¯¸ì§€ ì‚­ì œ ì‹œ ê´€ë ¨ ìƒíƒœë“¤ë„ í•¨ê»˜ ì •ë¦¬
  const removeImage = (uri: string) => {
    // ì„ íƒëœ ì´ë¯¸ì§€ ëª©ë¡ì—ì„œ ì œê±°
    setSelectedImages(prevImages => prevImages.filter(img => img.uri !== uri));
    // OCR ê²°ê³¼ì—ì„œ ì œê±°
    setOcrResults(prevResults => {
      const newResults = { ...prevResults };
      delete newResults[uri];
      return newResults;
    });
    // ì´ë¯¸ì§€ íƒ€ì… ì •ë³´ì—ì„œ ì œê±°
    setImageTypes(prev => {
      const newTypes = { ...prev };
      delete newTypes[uri];
      return newTypes;
    });
    
    // task ì œì•ˆì—ì„œë„ í•´ë‹¹ ì´ë¯¸ì§€ ì œê±°
    setImageTaskSuggestions(prev => {
      const newSuggestions = { ...prev };
      delete newSuggestions[uri];
      return newSuggestions;
    });

    // ì‚­ì œëœ ì´ë¯¸ì§€ê°€ í˜„ì¬ ì„ íƒëœ ì´ë¯¸ì§€ì˜€ë‹¤ë©´ ë‹¤ë¥¸ ì´ë¯¸ì§€ ì„ íƒ
    if (uri === selectedImageUri) {
      const remainingUris = Object.keys(imageTaskSuggestions).filter(key => key !== uri);
      setSelectedImageUri(remainingUris[0] || null);
    }
  };

  const openPreview = (mediaAsset: ImagePickerAsset) => {
    setPreviewMediaAsset(mediaAsset);
    Animated.timing(fadeAnim, {
      toValue: 1,
      duration: 300,
      useNativeDriver: false,
    }).start();
  };

  const closePreview = () => {
    setPreviewMediaAsset(null);
    Animated.timing(fadeAnim, {
      toValue: 0,
      duration: 300,
      useNativeDriver: false,
    }).start();
  };
  // ìŒì„± ì§ˆë¬¸ìœ¼ë¡œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
  const handleGetInfoFromSpeech = async (question: string) => {
    if (!question.trim()) {
      return;
    }
  
    setQuestionText(question); // ì§ˆë¬¸ í…ìŠ¤íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸
    setIsFetchingInfo(true);
    setInfoResult(null);
  
    try {
      const selectedMedia = previewMediaAsset;
      if (!selectedMedia || !selectedMedia.uri) {
        throw new Error('ë¯¸ë””ì–´ URIê°€ ì—†ìŠµë‹ˆë‹¤.');
      }
  
      let analysisText = '';
  
      // ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ì¸ ê²½ìš°
      if (selectedMedia.type === 'video') {
        try {
          const results = await extractTextFromVideo(selectedMedia.uri, 1);
          
          if (results.length > 0) {
            analysisText += '[ë¹„ë””ì˜¤ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼]\n';
            results.forEach(result => {
              analysisText += `[${result.time/1000}ì´ˆ] ${result.text}\n`;
            });
            analysisText += '\n';
          } else {
            analysisText += '[ë¹„ë””ì˜¤ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.]\n\n';
          }
        } catch (error) {
          console.error('ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜:', error);
          analysisText += '[ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.]\n\n';
        }
      } else {
        // ì´ë¯¸ì§€ ë¶„ì„ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼
        const analysisResult = await analyzeImage(selectedMedia.uri);
        if (!analysisResult) {
          throw new Error('ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
        }
  
        console.log('=== Image Analysis Started (Voice Query) ===');
        console.log('Analysis Result:', analysisResult);
  
        // ë¬¼ì²´ ê°ì§€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ìœ í˜• ì¶”ì •
        const detectedObjects = analysisResult.objects.map(obj => obj.name.toLowerCase());
        const detectedLabels = analysisResult.labels.map(label => label.description.toLowerCase());
        
        // ë¬¸ì„œ ìœ í˜• íŒë³„
        let documentType = '';
        if (detectedObjects.includes('receipt') || detectedLabels.includes('receipt')) {
          documentType = 'ì˜ìˆ˜ì¦';
        } else if (detectedObjects.includes('id card') || detectedLabels.includes('id card')) {
          documentType = 'ì‹ ë¶„ì¦';
        } else if (detectedObjects.includes('business card') || detectedLabels.includes('business card')) {
          documentType = 'ëª…í•¨';
        } else if (detectedObjects.includes('document') || detectedLabels.includes('document')) {
          documentType = 'ë¬¸ì„œ';
        }
  
        // ë¬¸ì„œ ìœ í˜•ì´ ê°ì§€ëœ ê²½ìš°
        if (documentType) {
          analysisText += `[ë¬¸ì„œ ìœ í˜•]\n${documentType}\n\n`;
        }
  
        // í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        if (analysisResult.text) {
          analysisText += `[í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼]\n${analysisResult.text}\n\n`;
        }
  
        // ì´ë¯¸ì§€ ë¼ë²¨
        if (analysisResult.labels.length > 0) {
          analysisText += '[ì´ë¯¸ì§€ ë¼ë²¨]\n';
          analysisResult.labels.forEach(label => {
            analysisText += `- ${label.description} (ì‹ ë¢°ë„: ${Math.round(label.confidence * 100)}%)\n`;
          });
          analysisText += '\n';
        }
      }
  
      // ìŒì„±ìœ¼ë¡œ ë°›ì€ ì§ˆë¬¸ ì¶”ê°€
      analysisText += `\nì§ˆë¬¸: ${question.trim()}`;
  
      const information = await getInfoFromTextWithOpenAI(analysisText);
      setInfoResult(information);
  
      Animated.timing(fadeAnim, {
        toValue: 1,
        duration: 500,
        useNativeDriver: false,
      }).start();
  
    } catch (error) {
      console.error('Error processing media with speech:', error);
      setInfoResult('ìŒì„± ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    } finally {
      setIsFetchingInfo(false);
      // ëª¨ë‹¬ ë‹«ê¸°
      closePreview();
    }
  };

  // Task ì„ íƒ ì‹œ í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
  const handleTaskSelect = async (task: TaskSuggestion) => {
    setQuestionText(task.task);
    setInfoResult(null);
    setIsFetchingInfo(true);

    try {
      // í˜„ì¬ ì„ íƒëœ ì´ë¯¸ì§€ ì°¾ê¸°
      const selectedMedia = selectedImages.find(img => img.uri === selectedImageUri);
      if (!selectedMedia?.uri) {
        throw new Error('ë¯¸ë””ì–´ URIê°€ ì—†ìŠµë‹ˆë‹¤.');
      }

      let analysisText = '';

      // ë¹„ë””ì˜¤ì¸ ê²½ìš° ë¹„ë””ì˜¤ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
      if (selectedMedia.type === 'video') {
        try {
          const results = await extractTextFromVideo(selectedMedia.uri, 1);
          if (results.length > 0) {
            analysisText = results.map(result => result.text).join('\n');
          }
        } catch (error) {
          console.error('ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜:', error);
        }
      } else {
        // ì´ë¯¸ì§€ì¸ ê²½ìš° ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
        const analysisResult = await analyzeImage(selectedMedia.uri);
        if (analysisResult && analysisResult.text) {
          analysisText = analysisResult.text;
        }
      }

      // ì„ íƒëœ taskì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ ê²€ìƒ‰
      const information = await getInfoFromTextWithOpenAI(`${analysisText}\n\nì§ˆë¬¸: ${task.task}`);
      setInfoResult(information);

      Animated.timing(fadeAnim, {
        toValue: 1,
        duration: 500,
        useNativeDriver: false,
      }).start();

    } catch (error) {
      console.error('Error processing task:', error);
      setInfoResult('ì‘ì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    } finally {
      setIsFetchingInfo(false);
      closePreview();
    }
  };

  return (
    <ScrollView
      style={styles.container}
      contentContainerStyle={styles.contentContainer}
      keyboardShouldPersistTaps="handled"
    >
      <BlurView intensity={20} style={styles.header}>
        <View style={styles.headerTextContainer}>
          <Text style={styles.title}>Findit!</Text>
          <Text style={styles.subtitle}>
            ë¯¸ë””ì–´ì—ì„œ{'\n'}
            ì •ë³´ë¥¼{'\n'}
            ì°¾ì•„ë³´ì„¸ìš”<Text style={{ color: '#46B876' }}>.</Text>{'\n'}
          </Text>
        </View>
      </BlurView>

      <View style={styles.summarySection}>
        <SummarizationSection
          questionText={questionText}
          setQuestionText={setQuestionText}
        />
        
        {selectedImages.length > 0 && (
          <View style={styles.imagesSection}>
            <ScrollView 
              horizontal 
              showsHorizontalScrollIndicator={false}
              contentContainerStyle={styles.imagesScrollContainer}
            >
              {selectedImages.map((media) => (
                <View key={media.assetId || media.uri} style={styles.imageWrapper}>
                  {media.type === 'video' ? (
                    <TouchableOpacity
                      onPress={() => setSelectedImageUri(prev => prev === media.uri ? null : media.uri)}
                      style={[
                        styles.imageTouchable,
                        selectedImageUri === media.uri && styles.selectedImageBorder
                      ]}
                    >
                      <VideoPreview
                        videoUri={media.uri}
                        onPress={() => handleMediaPreview(media)}
                      />
                    </TouchableOpacity>
                  ) : (
                    <TouchableOpacity
                      onPress={() => setSelectedImageUri(prev => prev === media.uri ? null : media.uri)}
                      style={[
                        styles.imageTouchable,
                        selectedImageUri === media.uri && styles.selectedImageBorder
                      ]}
                    >
                      {isLoadingOcr[media.uri] || isAnalyzing ? (
                        <BlurView intensity={90} style={styles.imageThumbnail}>
                          <Image 
                            source={{ uri: media.uri }} 
                            style={styles.imageThumbnail}
                            resizeMode="cover"
                          />
                        </BlurView>
                      ) : (
                        <Image 
                          source={{ uri: media.uri }} 
                          style={styles.imageThumbnail}
                          resizeMode="cover"
                        />
                      )}
                      {(isLoadingOcr[media.uri] || isAnalyzing) && (
                        <OcrLoadingAnimation />
                      )}
                    </TouchableOpacity>
                  )}
                  <View style={styles.imageButtonsContainer}>
                    <TouchableOpacity
                      style={styles.deleteButton}
                      onPress={() => removeImage(media.uri)}
                    >
                      <MaterialIcons name="close" size={20} color="#fff" />
                    </TouchableOpacity>
                    <TouchableOpacity
                      style={styles.magnifyButton}
                      onPress={() => handleMediaPreview(media)}
                    >
                      <MaterialIcons name="search" size={20} color="#fff" />
                    </TouchableOpacity>
                  </View>
                  <View style={{ marginTop: 12 }}>
                    <ImageTypeSelector 
                      uri={media.uri} 
                      currentType={imageTypes[media.uri] || 'OTHER'} 
                      onTypeChange={handleTypeChange} 
                    />
                  </View>
                </View>
              ))}
            </ScrollView>
          </View>
        )}
        
        {selectedImages.length === 0 && (
          <TouchableOpacity
            style={styles.imageUploadButton}
            onPress={showMediaOptions}
          >
            <MaterialIcons name="add" size={48} color="#8e8e8e" />
            <Text style={styles.imageUploadButtonText}>ë¯¸ë””ì–´ ì—…ë¡œë“œ</Text>
          </TouchableOpacity>
        )}

        <TouchableOpacity 
          style={[
            styles.getInfoButton, 
            (!questionText.trim() || !selectedImageUri) && styles.getInfoButtonDisabled,
            !questionText.trim() && selectedImageUri && styles.getInfoButtonEnabled
          ]} 
          onPressIn={handleStartRecording}
          onPressOut={handleStopRecording}
          disabled={isFetchingInfo || isProcessingSpeech || isProcessingAI || !selectedImageUri}
        >
          {isFetchingInfo ? (
            <LoadingWave />
          ) : isRecording ? (
            <View style={styles.recordingContainer}>
              <MaterialIcons name="mic" size={24} color="#fff" />
              <Text style={styles.getInfoButtonText}>ìŒì„± ì¸ì‹ ì¤‘...</Text>
            </View>
          ) : isProcessingSpeech || isProcessingAI ? (
            <LoadingWave />
          ) : (
            <Text style={[
              styles.getInfoButtonText,
              (!questionText.trim() || !selectedImageUri) && styles.getInfoButtonTextDisabled,
              !questionText.trim() && selectedImageUri && styles.getInfoButtonTextEnabled
            ]}>
              {!questionText.trim() ? "ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°" : "ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"}
            </Text>
          )}
        </TouchableOpacity>

        {/* Task Suggestions Section - Only show when an image is selected and suggestions exist */}
        {selectedImageUri && imageTaskSuggestions[selectedImageUri] && (
          <View style={styles.taskSuggestionsContainer}>
            <TaskSuggestionList
              suggestions={imageTaskSuggestions[selectedImageUri].suggestions}
              onTaskSelect={handleTaskSelect}
            />
          </View>
        )}

        {/* Answer Display */}
        {isFetchingInfo ? (
          <AnswerLoadingSkeleton />
        ) : infoResult && (
          <View>
            <Text style={styles.infoContainerTitle}>ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼</Text>
            <View style={styles.infoResultContainer}>
              <ScrollView style={styles.infoResultScrollView}>
                <Markdown style={markdownStyles}>
                  {infoResult}
                </Markdown>
              </ScrollView>
            </View>
          </View>
        )}
      </View>

      <MediaPreviewModal
        visible={!!previewMediaAsset}
        onClose={closePreview}
        mediaAsset={previewMediaAsset}
        ocrResult={ocrResults[previewMediaAsset?.uri || '']}
        isLoadingOcr={isLoadingOcr[previewMediaAsset?.uri || '']}
        colorScheme={colorScheme === 'dark' ? 'dark' : 'light'}
        searchTerm={searchTerm}
        setSearchTerm={setSearchTerm}
        analysisResult={analysisResult}

      >
  <Text>ì´ë¯¸ì§€ ìœ í˜•: {imageTypes[previewMediaAsset?.uri || ''] || 'ê¸°íƒ€'}</Text>
</MediaPreviewModal>
    </ScrollView>
  );
};

export default HomeScreen;
